{
  "epochs": 1,
  "grad_accumulation": 1,
  "gen_length": 400,
  "batch_size": 2,
  "test_batch_size": 8,
  "lr": 1e-05,
  "weight_decay": 0.01,
  "warm_up_steps": 100,
  "model_dir": "checkpoints/",
  "log_divisor": 100,
  "save_divisor": 5,
  "exp_name": "testrun",
  "optimizer": "Adam",
  "scheduler": "linear",
  "precision": "bf16",
  "model_name": "Qwen/Qwen2.5-3B",
  "max_length": 650,
  "n_shot": 6,
  "self_consistency": 0,
  "delete_model_after_loading": true,
  "accumulate": true,
  "task": "mate",
  "inference_temp": 1.0,
  "no_hint": true,
  "base_model_path": "/root/27th-conference-LLMResearch/star/LLM_Research/checkpoints/final_model.pth"
}
